{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basic transfer learning with cats and dogs data"
      ],
      "metadata": {
        "id": "PABRmIt62Cpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import tensorflow\n",
        "The code snippet you provided is intended for Google Colab, where you can switch between different versions of TensorFlow using the %tensorflow_version magic command. This command is only available in Colab, and attempting to use it outside Colab will raise an exception.\n",
        "\n",
        "Here’s what happens:\n",
        "\n",
        "try block: This attempts to use the %tensorflow_version 2.x magic command to ensure that TensorFlow version 2.x is being used.\n",
        "\n",
        "except block: If the code is run outside of Colab (where %tensorflow_version doesn't exist), it catches the exception and prevents the code from breaking."
      ],
      "metadata": {
        "id": "3SUORz8t2J_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvawKFaT2RvX",
        "outputId": "99c654a3-bd50-46ad-e77f-9eb42afee202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import modules and download the cats and dogs dataset.\n",
        "\n",
        "The code you provided downloads a dataset of cats and dogs from the internet, extracts it, and prepares it for further processing in a deep learning task. Here’s a breakdown of what each part of the code does:\n",
        "\n",
        "Step-by-step Explanation:\n",
        "Importing libraries:\n",
        "\n",
        "\n",
        "*   urllib.request: Used to download files from a URL.\n",
        "*   os: Used to interact with the operating system, such as handling directories and files.\n",
        "\n",
        "*   zipfile: Used to extract .zip files.\n",
        "*   random: Can be used for generating random numbers or selecting random items.\n",
        "\n",
        "*   ImageDataGenerator, layers, Model: These are parts of the TensorFlow/Keras library for creating models and augmenting image datasets.\n",
        "*   InceptionV3: A pre-trained model from TensorFlow.\n",
        "\n",
        "*   RMSprop: An optimizer from TensorFlow.\n",
        "*  shutil.copyfile: Used to copy files.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   data_url: The link to the dataset (a .zip file) containing images of cats and dogs.\n",
        "*   urllib.request.urlretrieve: Downloads the dataset and saves it as \"catsdogs.zip\" in the current working directory.\n",
        "\n",
        "*   zipfile.ZipFile: Opens the downloaded zip file.\n",
        "*   extractall(download_dir): Extracts the contents of the zip file into a directory (in this case, /tmp/)\n"
      ],
      "metadata": {
        "id": "j1lM7QPZkAm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from shutil import copyfile\n",
        "\n",
        "\n",
        "data_url = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\"\n",
        "data_file_name = \"catsdogs.zip\"\n",
        "download_dir = '/tmp/'\n",
        "urllib.request.urlretrieve(data_url, data_file_name)\n",
        "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
        "zip_ref.extractall(download_dir)\n",
        "zip_ref.close()\n"
      ],
      "metadata": {
        "id": "vVkodXgFkFAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that the dataset has the expected number of examples."
      ],
      "metadata": {
        "id": "gHrFz9D6lWfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of cat images:\",len(os.listdir('/tmp/PetImages/Cat/')))\n",
        "print(\"Number of dog images:\", len(os.listdir('/tmp/PetImages/Dog/')))\n",
        "\n",
        "# Expected Output:\n",
        "# Number of cat images: 12501\n",
        "# Number of dog images: 12501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SGP6R5ylYn2",
        "outputId": "bb6837bb-a217-476c-e1d7-01be3916df5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of cat images: 12501\n",
            "Number of dog images: 12501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create some folders that will store the training and test data.\n",
        "- There will be a training folder and a testing folder.\n",
        "- Each of these will have a subfolder for cats and another subfolder for dogs.\n",
        "\n",
        "Here is the directory folders explanation\n",
        "- /tmp/cats-v-dogs/: The main directory.\n",
        "- /tmp/cats-v-dogs/training/: Directory to store training data.\n",
        "- /tmp/cats-v-dogs/testing/: Directory to store testing data.\n",
        "- /tmp/cats-v-dogs/training/cats: Subdirectory for training images of cats.\n",
        "- /tmp/cats-v-dogs/training/dogs: Subdirectory for training images of dogs.\n",
        "- /tmp/cats-v-dogs/testing/cats: Subdirectory for testing images of cats.\n",
        "- /tmp/cats-v-dogs/testing/dogs: Subdirectory for testing images of dogs."
      ],
      "metadata": {
        "id": "bfFEnWkwqJpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    os.mkdir('/tmp/cats-v-dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n",
        "except OSError:\n",
        "    pass"
      ],
      "metadata": {
        "id": "T5FhG96Eqf1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data into training and test sets\n",
        "\n",
        "- The following code put first checks if an image file is empty (zero length)\n",
        "- Of the files that are not empty, it puts 90% of the data into the training set, and 10% into the test set."
      ],
      "metadata": {
        "id": "nHv5SPauqoca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from shutil import copyfile\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    files = []\n",
        "    '''filename contains just the name of the file (e.g., cat1.jpg), not the full path to it.\n",
        "      To access the file, you need the full path, which is the directory (SOURCE) plus the file name.\n",
        "      SOURCE is the path to the directory (e.g., /tmp/cats-v-dogs/training/cats/), and filename is the file name (e.g., cat1.jpg).\n",
        "      When you concatenate SOURCE + filename, the result is the full path: /tmp/cats-v-dogs/training/cats/cat1.jpg.\n",
        "    '''\n",
        "    for filename in os.listdir(SOURCE):\n",
        "        file = SOURCE + filename\n",
        "        if os.path.getsize(file) > 0:\n",
        "            files.append(filename)\n",
        "        else:\n",
        "            print(filename + \" is zero length, so ignoring.\")\n",
        "\n",
        "    training_length = int(len(files) * SPLIT_SIZE)\n",
        "    testing_length = int(len(files) - training_length)\n",
        "    '''By passing the length of the list, you're effectively asking\n",
        "    random.sample() to return all the elements of files, but in a randomly shuffled order.\n",
        "    '''\n",
        "    shuffled_set = random.sample(files, len(files))\n",
        "    training_set = shuffled_set[0:training_length]\n",
        "    testing_set = shuffled_set[training_length:]\n",
        "\n",
        "    for filename in training_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TRAINING + filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "    '''It doesn't move the file but creates a new copy of the file at the\n",
        "    destination. The file cat1.jpg remains in the original source folder,\n",
        "    and another cat1.jpg is created in the destination folder.\n",
        "    '''\n",
        "    for filename in testing_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TESTING + filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "\n",
        "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
        "TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n",
        "TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n",
        "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
        "TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n",
        "TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n",
        "\n",
        "split_size = .9\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n",
        "\n",
        "# Expected output\n",
        "# 666.jpg is zero length, so ignoring\n",
        "# 11702.jpg is zero length, so ignoring"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiLbzMsQqtB3",
        "outputId": "4f356528-1976-40d6-987d-3631d27e0a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "666.jpg is zero length, so ignoring.\n",
            "11702.jpg is zero length, so ignoring.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that the training and test sets are the expected lengths."
      ],
      "metadata": {
        "id": "6O9JYv9I1grm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of training cat images\", len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n",
        "print(\"Number of training dog images\", len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n",
        "print(\"Number of testing cat images\", len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n",
        "print(\"Number of testing dog images\", len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n",
        "\n",
        "# expected output\n",
        "# Number of training cat images 11250\n",
        "# Number of training dog images 11250\n",
        "# Number of testing cat images 1250\n",
        "# Number of testing dog images 1250"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j14jq_W21jIY",
        "outputId": "2cc3ffff-a3b7-4a3a-d4ff-ea5cbbce17eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training cat images 11250\n",
            "Number of training dog images 11250\n",
            "Number of testing cat images 1250\n",
            "Number of testing dog images 1250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation (try adjusting the parameters)!\n",
        "\n",
        "Here, you'll use the `ImageDataGenerator` to perform data augmentation.  \n",
        "- Things like rotating and flipping the existing images allows you to generate training data that is more varied, and can help the model generalize better during training.  \n",
        "- You can also use the data generator to apply data augmentation to the validation set.\n",
        "\n",
        "You can use the default parameter values for a first pass through this lab.\n",
        "- Later, try to experiment with the parameters of `ImageDataGenerator` to improve the model's performance.\n",
        "- Try to drive reach 99.9% validation accuracy or better."
      ],
      "metadata": {
        "id": "Ri665SyMxTb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n",
        "# Experiment with your own parameters to reach 99.9% validation accuracy or better\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "                                                    batch_size=100,\n",
        "                                                    class_mode='binary',\n",
        "                                                    target_size=(150, 150))\n",
        "\n",
        "VALIDATION_DIR = \"/tmp/cats-v-dogs/testing/\"\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n",
        "                                                              batch_size=100,\n",
        "                                                              class_mode='binary',\n",
        "                                                              target_size=(150, 150))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDjmkXLQxXrc",
        "outputId": "da432820-d7aa-4f8c-ed47-e7ee6c7874fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 22498 images belonging to 2 classes.\n",
            "Found 2500 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get and prepare the model\n",
        "\n",
        "You'll be using the `InceptionV3` model.  \n",
        "- Since you're making use of transfer learning, you'll load the pre-trained weights of the model.\n",
        "- You'll also freeze the existing layers so that they aren't trained on your downstream task with the cats and dogs data.\n",
        "- You'll also get a reference to the last layer, 'mixed7' because you'll add some layers after this last layer.\n",
        "\n",
        "Adjustments of InceptionV3 model:\n",
        "\n",
        "- You're excluding the top (fully connected) layers by setting include_top=False. This is common in transfer learning, where you reuse the convolutional layers as a feature extractor.\n",
        "- You're specifying your own input shape of (150, 150, 3), meaning your input images are 150x150 pixels with 3 color channels (RGB).\n",
        "- You're not loading any pre-trained weights by setting weights=None initially, but later, you load specific pre-trained weights using load_weights().\n",
        "\n",
        "Explanation\n",
        "\n",
        "- urllib.request.urlretrieve(): This function from Python's urllib library downloads a file from the given URL (in this case, the weights URL).\n",
        "- weights_url: The source URL where the file will be downloaded from.\n",
        "- weights_file: The name of the local file where the downloaded content will be saved.\n",
        "- pre_trained_model.get_layer('mixed7'): The get_layer() method allows you to retrieve a specific layer from the pre-trained model by name. In this case, you're selecting the \"mixed7\" layer from the InceptionV3 architecture.\n",
        "- InceptionV3 has multiple layers, and \"mixed7\" is one of them, specifically a convolutional block that's used for feature extraction.\n",
        "- last_layer.output_shape: This prints the shape of the output from the \"mixed7\" layer. The output shape is typically in the form (batch_size, height, width, channels) or (None, height, width, channels), where None represents a flexible batch size.\n",
        "- last_layer.output: This captures the actual output (the feature map) from the \"mixed7\" layer.\n",
        "This feature map will now be used as the input to the additional layers you will add to the model. You will typically add your custom fully connected layers (or other types of layers) on top of this output to fine-tune the model for your specific task (e.g., classifying images as cats or dogs).\n",
        "\n"
      ],
      "metadata": {
        "id": "qgIZihgE9PoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_url = \"https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
        "weights_file = \"inception_v3.h5\"\n",
        "urllib.request.urlretrieve(weights_url, weights_file)\n",
        "\n",
        "# Instantiate the model\n",
        "pre_trained_model = InceptionV3(input_shape=(150, 150, 3),\n",
        "                                include_top=False,\n",
        "                                weights=None)\n",
        "\n",
        "# load pre-trained weights\n",
        "pre_trained_model.load_weights(weights_file)\n",
        "\n",
        "# freeze the layers\n",
        "for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# pre_trained_model.summary()\n",
        "\n",
        "last_layer = pre_trained_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output.shape)\n",
        "last_output = last_layer.output\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKaQ9-8V94c6",
        "outputId": "ce0ac090-5f6a-4488-a39e-c209ab6e50eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "last layer output shape:  (None, 7, 7, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add layers\n",
        "Add some layers that you will train on the cats and dogs data.\n",
        "- `Flatten`: This will take the output of the `last_layer` and flatten it to a vector.\n",
        "- `Dense`: You'll add a dense layer with a relu activation.\n",
        "- `Dense`: After that, add a dense layer with a sigmoid activation.  The sigmoid will scale the output to range from 0 to 1, and allow you to interpret the output as a prediction between two categories (cats or dogs).\n",
        "\n",
        "Explanation\n",
        "- x = layers.Flatten()(last_output): The output of the last layer from the pre-trained model (last_output) is a 3D feature map. The Flatten layer converts this 3D output into a 1D vector so it can be used by fully connected (dense) layers. This step is essential because dense layers expect 1D input.\n",
        "- The ReLU (Rectified Linear Unit) activation introduces non-linearity into the model, allowing it to learn more complex patterns. The 1,024 units make this a relatively large and powerful layer.\n",
        "-model = Model(pre_trained_model.input, x):\n",
        " You're defining the final model that uses the input of the pre-trained InceptionV3 model and adds your new layers on top.\n",
        "The input is passed through both the pre-trained InceptionV3 layers and your custom layers, creating the final architecture for training."
      ],
      "metadata": {
        "id": "f2MrWsMDY9xP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the output layer to 1 dimension\n",
        "x = layers.Flatten()(last_output)\n",
        "# Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "x = layers.Dense(1024, activation='relu')(x)\n",
        "# Add a final sigmoid layer for classification\n",
        "x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(pre_trained_model.input, x)\n"
      ],
      "metadata": {
        "id": "EJBR1-poZBui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "Compile the model, and then train it on the test data using `model.fit`\n",
        "- Feel free to adjust the number of epochs.  This project was originally designed with 20 epochs.\n",
        "- For the sake of time, you can use fewer epochs (2) to see how the code runs.\n",
        "- You can ignore the warnings about some of the images having corrupt EXIF data. Those will be skipped."
      ],
      "metadata": {
        "id": "hLFa_g7JgQXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# compile the model\n",
        "model.compile(optimizer=RMSprop(learning_rate=0.0001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "# train the model (adjust the number of epochs from 1 to improve performance)\n",
        "history = model.fit(\n",
        "            train_generator,\n",
        "            validation_data=validation_generator,\n",
        "            epochs=2,\n",
        "            verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiHGlwzygUqm",
        "outputId": "8f669852-934d-4586-f197-aec837cabfcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 740ms/step - acc: 0.8718 - loss: 0.3654"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:900: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 774ms/step - acc: 0.8720 - loss: 0.3647 - val_acc: 0.9736 - val_loss: 0.0725\n",
            "Epoch 2/2\n",
            "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 662ms/step - acc: 0.9385 - loss: 0.1421 - val_acc: 0.9744 - val_loss: 0.0707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the training and validation accuracy\n",
        "\n",
        "You can see how the training and validation accuracy change with each epoch on an x-y plot."
      ],
      "metadata": {
        "id": "PAAQUdj8jFzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['acc']\n",
        "val_acc=history.history['val_acc']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "4mJgp2v7jLpM",
        "outputId": "daa7e999-a3f3-4966-ab93-210f7b3d1738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict on a test image\n",
        "\n",
        "You can upload any image and have the model predict whether it's a dog or a cat.\n",
        "- Find an image of a dog or cat\n",
        "- Run the following code cell.  It will ask you to upload an image.\n",
        "- The model will print \"is a dog\" or \"is a cat\" depending on the model's prediction."
      ],
      "metadata": {
        "id": "lo2eEHjTlNAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "\n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = load_img(path, target_size=(150, 150))\n",
        "  x = img_to_array(img)\n",
        "  x /= 255\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  image_tensor = np.vstack([x])\n",
        "  classes = model.predict(image_tensor)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a dog\")\n",
        "  else:\n",
        "    print(fn + \" is a cat\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "VuoAhroDlQMt",
        "outputId": "05c41b68-e653-4422-99c6-fa43e2a94e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a7e9e3f6-bfff-49b6-9e96-d952e2f4da6d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a7e9e3f6-bfff-49b6-9e96-d952e2f4da6d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Screenshot (386).png to Screenshot (386).png\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
            "[0.35617608]\n",
            "Screenshot (386).png is a cat\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}